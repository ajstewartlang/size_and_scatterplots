---
author_1: "Gabriel Strain"
author_2: "Andrew J. Stewart"
author_3: "Paul Warren"
author_4: "Caroline Jay"
author_1_email: "Gabriel.Strain@manchester.ac.uk"
author_2_email: "Andrew.J.Stewart@manchester.ac.uk"
author_3_email: "Paul.Warren@manchester.ac.uk"
author_4_email: "Caroline.Jay@manchester.ac.uk"
affiliation: "The University of Manchester"
acknowledgements: "lah di dah"
output:
  bookdown::pdf_book: # for automatic figure-numbering (https://bookdown.org/yihui/rmarkdown-cookbook/figure-number.html)
    keep_tex: yes
    template: template.tex
    citation_package: natbib
    fig_crop: no
title: "Point Size and Correlation Perception in Scatterplots"
editor_options: 
  markdown: 
    wrap: 72
bibliography: size_contrast_scatterplots.bib
abstract: |
  Place abstract here.
introduction: |
  Place intro here. Don't forget to put hypotheses
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)
# Knitting this document requires tinytex (install.packages("tinytex"))
# For formatting to be correct, additional tinytex packages are required
# Run tinytex:::install_yihui_pkgs() before knitting
```

```{r, include=FALSE}
set.seed(1234) # seed for all random number generation

# Loading packages
library(rticles)
library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(tinylabels)
library(ggdist)
library(ggpubr)
library(pwr)
library(geomtextpath)
library(conflicted)

# tell R to use dplyr::select() instead of MASS::select()

conflicts_prefer(dplyr::select(), dplyr::filter())

# Comment out the following line if additional tinytex packages are already installed
# tinytex:::install_yihui_pkgs()
```

```{r eval-models, include=FALSE}
# in this script, models are cached. If eval_models <- FALSE, script will load
# cached models. Set eval_models <- TRUE to rebuild models from scratch
eval_models <- TRUE

if (eval_models == FALSE){
  lazyload_cache_dir('size_contrast_scatterplots/latex')
}
```

```{r load-data, include=FALSE}
# load in data files
size_anon <- read_csv("data/final_data.csv")
```

```{r wrangle, include = FALSE}
# function for wrangling data

# first do literacy


wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process visual threshold testing
  
  visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels", "participant", "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels, pattern = "vis_threshold_plots/", replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer, pattern = "_VT.png", replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    select("VT_no_correct", "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
  monitor_information <- anon_file %>%
    filter(!is.na(height)) %>%
    filter(!is.na(res_width)) %>%
    mutate(res_height = res_width*0.5625,
           width = height*0.5625,
           dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
        select(c("dot_pitch", "participant", "res_width"))
    
  
# extract demographic information
# link slider response numbers to gender categories
  
  demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split plots_with_labels column into item and contrast condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-A")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-B")) %>%
  mutate(images = str_replace(images, pattern = "C", replacement = "-C")) %>%
  mutate(images = str_replace(images, pattern = "D", replacement = "-D")) %>%
  separate(images, c("item", "size"), sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
                  "item",
                  "size",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "session")) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "size")), as_factor)) %>%
  select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(contrast = fct_relevel(size, c('A', 'B', 'C', 'D'))) %>% 
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data files 

wrangle(size_anon)

# remove anon df from environment

rm(size_anon)

# extract age data

age <- distinct(exp_size_only_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract gender data

gender <- distinct(exp_size_only_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data

literacy <- distinct(exp_size_only_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))



```

```{r comparison-function, include=FALSE}
# this function takes a model and creates a nested model with the fixed effects 
# term removed for anova comparison
comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r anova-results-function, include=FALSE}
# this function takes two nested models, runs an anova, and the outputs the 
# Chi-square statistic, the degrees of freedom, and the p value to the global environment
anova_results <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}
```

```{r contrasts-extract, echo=FALSE}
# this function extracts test statistics and p values from model summaries
contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ size)
  
  params <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(params)
  
}
```

```{r sum-stats-extract, echo = FALSE}
# generates summary statistics (mean and standard deviation)
sum_stats <- function(df) {
  
  stats <- df %>%
    filter(!is.na(difference)) %>%
    group_by(size) %>%
    summarise(mean = mean(difference),
              sd = sd(difference)) %>%
    rename_with(str_to_title) %>%
    rename_with(str_to_upper, starts_with("S"))
  
  return(stats)
  
}
```

```{r effect-size-function, include = FALSE}
get_effects_sizes <- function(model, d) {
  
  effect_sizes <- lme.dscore(model, data = d, type = "lme4")
  
  effects_df <- as.data.frame(effect_sizes[3])
  
  return(effects_df)
}
```

```{r dot-plot-function, include = FALSE}
dot_plot_function <- function(df) {

data <- df %>%
  group_by(size) %>%
  filter(!is.na(difference)) %>%
  filter(!is.na(size)) %>%
  summarise(
    mean = mean(difference),
    lci = t.test(difference, conf.level = 0.95)$conf.int[1],
    hci = t.test(difference, conf.level = 0.95)$conf.int[2],
  )

  data %>%
    mutate(size = fct_relevel(size, "A", "B", "C", "D")) %>%
    ggplot(aes(x = size, y = mean)) +
    geom_point() +
    #coord_cartesian(ylim = c(0, max_error))
    geom_errorbar(aes(ymin=lci, ymax=hci), colour="black", width=0.025, linewidth =1) +
    theme_ggdist() +
    labs(x = "Point Size Condition",
         y = "Mean Error") +
    theme(axis.text = element_text(size = 13),
          axis.title = element_text(size = 16))
}
```

```{r plot-sd-sd, include = FALSE}
plot_sd_sd_function <- function(df){
  df %>% 
  summarise(ppt_sd = sd(difference)) %>% 
  drop_na() %>% 
  group_by(size) %>% 
  summarise(mean_sd = mean(ppt_sd), sd_sd = sd(ppt_sd)) %>% 
  ggplot(aes(x = size, y = mean_sd)) +
  geom_point(size = 2) + 
  geom_errorbar(aes(ymin = mean_sd - sd_sd, ymax = mean_sd + sd_sd), width = 0.05, linewidth = 1) + 
  theme_ggdist() +
  ylim(0, .3) +
  labs(x = "Point Size Condition",
       y = "Mean Standard Deviation") +
        theme(axis.text = element_text(size = 13),
              axis.title = element_text(size = 16))
}
```

```{r error-bar-plot, include = FALSE}
plot_error_bars_function <- function(df, measure, l){
  df %>% 
  drop_na() %>% 
  group_by(size, my_rs) %>% 
  summarise(sd = sd(get(measure)), mean = mean(get(measure))) %>% 
  ggplot(aes(x = my_rs, y = mean)) +
  geom_point(size = 0.2) + 
  geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd),width = 0.01, size = 0.3) +
  theme_ggdist() +
  scale_y_continuous(breaks = seq(0,1, 0.2)) +
  theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1) +
  facet_wrap(size ~., ncol = 4, labeller = labeller(size = l)) +
    labs(x = "Objective r",
         y = "Mean r estimation") +
    xlim(0.2,1)
}

```

```{r labeller, include = FALSE}
labels_size <- c(A = "Non-Linear Decay", B = "Linear Decay", C = "Inverted Decay", D = "Standard Size")
```

```{r example-plots, include = FALSE}
example_plots <- function () {
  
  set.seed(1234)
  
  my_sample_size = 128
  
  my_desired_r = 0.6
  
  mean_variable_1 = 0
  sd_variable_1 = 1
  
  mean_variable_2 = 0
  sd_variable_2 = 1
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
slopes <- data_with_resid %>%
  mutate(slope_linear = my_residuals/3.2) %>%
  mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
  mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1)
  
plot_example_function <- function (d, x, t) {
  
  set.seed(1234)
  
  ggplot(d, aes(x = V1, y = V2)) +
  scale_size_identity() +
  geom_point(aes(size = 4*(x + 0.2)), shape = 16)  +
  labs(x = "", y = "") +
  theme_classic() +
  theme(axis.text = element_blank(),
        plot.margin = unit(c(0,0,0,0), "cm"),
        legend.position = "none",
        plot.title = element_text(size = 15)) +
  labs(title = t)

}  

plots <- ggarrange(plot_example_function(slopes, (1-slopes$slope_0.25), "Non-linear Decay (b = 0.25)"),
                   plot_example_function(slopes, (1-slopes$slope_linear), "Linear Decay"),
                   plot_example_function(slopes, (1-slopes$slope_inverted), "Inverted Non-linear Decay"),
                   plot_example_function(slopes, 0.05, "Standard Size"))

return(plots)

}
```

put ethics here

# Related Work

## Correlation Perception

see Strain et al for a brief review of the history of correlation perception
testing with scatterplots

## Point Size

Hong et al paper will be useful here

## Dot Pitch and Crowdsourced Experiments

# Methodology

## Open Research Statement

The experiment was conducted according to the principles of open and reproducible research.
All data and analysis code are available at https://github.com/gjpstrain/size_contrast_and_scatterplots.
This repository contains instructions for building a docker image to fully 
reproduce the computational environment used, allowing for full replications
of stimulus generation, analyses, and the paper itself. The experiment was 
pre-registered with the OSF (https://osf.io/k4gd8).

## Participants

150 participants were recruited using the Prolific.co platform. Normal to
corrected-to-normal vision and English fluency were required for participation. As in
\cite{strain_2023}, and in accordance with previously published guidelines \cite{peer_2021},
participants were required to have completed at least 100 studies on Prolific, and were
required to have a Prolific score of at least 100, indicating acceptance on at least
100/101 previously completed studies. Participants who took part in any of our 
previous studies were prevented from participating, and participants were only
permitted to complete the experiment on a desktop or laptop computer.

Data were collected from 164 participants. 14 failed more than 2 out of 6 attention
check questions, and, as per pre-registration stipulations, were rejected from the study. Data
from 150 participants was included in the analysis (`r printnum(gender$M)`% male, `r printnum(gender$F)`% female, and 
`r printnum(gender$NB)`% non-binary). Mean age of participants was `r printnum(age$mean)`
(*SD* = `r printnum(age$sd)`). Mean graph literacy score was `r printnum(literacy$mean)`
(*SD* = `r printnum(literacy$sd)`) out of 30. The average time taken to complete
the experiment was 39 minutes (SD = 14 minutes).

## Stimuli

The data used to generated the scatterplots in the current study was identical to that
used previously \cite{strain_2023}. Scatterplots were generated based on 45 uniformly distributed *r* values
between 0.2 and 0.99. Scatterplot points were generated based on bivariate normal 
distributions with standard deviations of 1 in each direction. Each scatterplot
had a 1:1 aspect ratio, was generated as a 1200 x 1200 pixel .png image, and was
scaled up or down according to the participant's monitor. See section \ref{dot-pitch-and-crowdsourced-experiments}
for a more detailed discussion of precise point sizes and dot pitch in crowd-sourced
experiments.

As in our previous study \cite{strain_2023}, we used equation 1 to map residuals 
to point sizes. We used a scaling factor of 4 and a constant of 0.2 to achieve a
minimum point size of 12/13 pixels, which is consistent with the point size on
a 1920 x 1080 monitor for both experiments in \cite{strain_2023}. Again, see section \ref{dot-pitch-and-crowdsourced-experiments}
for a discussion of dot pitch. Scripts detailing scatterplot and mask generation
can be found in the item preparation folder in the repository linked below.

\begin{equation}
  point-size = 1 - b^R
\end{equation}

## Dot Pitch and Crowdsourced Experiments

```{r dot-pitch, include = FALSE}
mean_dot_pitch <- mean(exp_size_only_tidy$dot_pitch)

sd_dot_pitch <- sd(exp_size_only_tidy$dot_pitch)
```

In our previous study \cite{strain_2023}, we had no way of obtaining dot pitch
or participant to monitor distance due to the online, crowdsourced nature of the 
experiments. Since then we have adopted a method for obtaining the height of a 
participant's monitor in inches \cite{screenscale}. Combining this with the
monitor resolution fetched from Psychopy and assuming a widescreen 16:9 aspect ratio
allows us to infer dot pitch and therefore the physical size of the points in our
experiment. Mean dot pitch was `r printnum(mean_dot_pitch)`mm, ($SD = `r printnum(sd_dot_pitch)`$),
corresponding to a physical size on the screen of `r printnum(mean_dot_pitch*13)`mm
for the smallest points displayed. While dot pitch is not necessary information for the present study, as we are only
interested in the relative differences in correlation estimates between point size
conditions, the fact that we can collect it at all is indicative of the gap
being narrowed with regards to psychophysical testing between in-person and online
experiments. Given that the latter are cheaper, easier, and immeasurably quicker,
anything that can be done be done to narrow this gap in capability is a boon.
See section \ref{results} for analyses including dot pitch as a predictor.

## Visual Threshold Testing

```{r threshold-values, include=FALSE}
vis_df <- exp_size_only_tidy %>%
  group_by(VT_no_correct) %>%
  count()
```

It is key that our manipulation does not functionally removing data from the scatterplot,
thus, in order to test that all our points were visible across a range of viewing
contexts and on a range of apparatus, we included visual threshold testing prior
to the experimental items in the study. Participants were shown six scatterplots
with a number of points, and were asked to enter in a textbox how many points
were being displayed. The points were the same size as the smallest points used
in the experimental materials. `r printnum(vis_df$n[1]/270, digits = 0)`% of 
participants were correct on `r printnum(vis_df$VT_no_correct[1])` out of 6 visual
threshold questions, while `r printnum(vis_df$n[2]/270, digits = 0)`% were correct
on `r printnum(vis_df$VT_no_correct[2])` out of 6. It should be noted that those 
participants scoring 5/6 did not answer incorrectly, rather did not answer
at all for this particular questions, which is more suggestive of either
a mis-click or an initial misunderstanding of the task they were asked to complete.
Regardless, we consider these results to be indicative of a sufficient level of 
point visibility for the current experiment.

## Design

The experiment used a fully repeated measures, within-participants design, with each
participant seeing and responding to each of the 180 scatterplots in a randomised order.
There were four scatterplots for each of the 45 *r* values corresponding to the
four levels of the size condition, examples of which can be see in figure \ref{fig:examples}.
Everything needed to run the experiment, including code, materials, instructions, and scripts, is
hosted at https://gitlab.pavlovia.org/Strain/exp_size_only.

```{r examples, warning=FALSE, echo=FALSE, message=FALSE, fig.asp=1, fig.show='hold', fig.cap="Four levels of the point size condition, demonstrated with an \\textit{r} value of 0.6", out.width="100%"}
example_plots()
```

## Procedure

Each participants was shown the participants information sheet (PIS) and provided
consent through key presses in response to consent statements. They were asked
to provide their age in a free text box, and their gender identity. Participants
then completed the 5-item Subjective Graph Literacy (SGL) test \cite{garcia_2016}, 
followed by the visual threshold testing described above. Participants then completed
the screen scaling task described in section \ref{dot-pitch-and-crowdsourced-experiments}.
Participants were given instructions, and then shown examples of *r* = 0.2, 0.5, 0.8, and
0.95, as in our previous work \cite{strain_2023}. Section \ref{training} includes 
a discussion of the potential effects of this training. Two practice trials were
given before the experiment began. Participants worked through a series of 180 trials
in which they were asked to use a slider to estimate the correlation shown in
the scatterplot. Interspersed were six attention check trials which asked 
participants to set the slider to 1 or 0 and ignore the scatterplot.

# Results

All analyses were conducted using R (version 4.2.3 \cite{r_core}). Models were
built using the **buildmer** (version 2.8 \cite{voeten_buildmer_2022}) and **lme4**
(version 1.1-32 \cite{bates_lme4_2015}) packages, with size manipulation being set
as the predictor for participants' errors in correlation estimates

```{r dot-plot, warning=FALSE, echo=FALSE, message=FALSE, fig.show='hold', fig.cap="Mean error in correlation estimates across the four size manipulation conditions, with 95\\% confidence intervals shown.", out.width="100%"}
dot_plot_function(exp_size_only_tidy) +
  scale_x_discrete(labels = c("Non-linear\nSize Decay",
                              "Linear Size\nDecay",
                              "Inverted Non-linear\nSize Decay",
                              "Standard\nSize")) +
  ylim(0,0.2)
```

```{r model, cache=eval_models, cache.comments=FALSE, eval=eval_models, message=FALSE, warning=FALSE, include=FALSE, cache.path="size_and_scatterplots/latex/"}
size_model <- buildmer(difference ~ size +
                         (1 + size | participant) +
                         (1 + size | item),
                       data = exp_size_only_tidy)
```

```{r model-comparison,cache=eval_models, cache.comments=FALSE, eval=eval_models, message=FALSE, warning=FALSE, include=FALSE, cache.path="size_and_scatterplots/latex"}
model_comparison <- comparison(size_model)
```

```{r anova, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
anova_results(size_model, model_comparison)
```

```{r effects-sizes, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
effects_df <- get_effects_sizes(model, exp_size_only_tidy)
```

Mean errors in correlation estimates for the four size manipulation conditions
can be seen in figure \ref{fig:dot-plot}. A likelihood ratio test revealed that the
model including size manipulation as a predictor explained significantly more
variance than a null model ($\chi^2$(`r in_paren(size_model.df)`) = `r printnum(size_model.Chisq)`,
*p* `r printp(size_model.p, add_equals = TRUE)`). This model has random intercepts for
items and participants. The effect here is driven by participants' errors being lower 
for scatterplots with the non-linear size decay manipulation than for all other conditions,
for error being lower for scatterplots with linear size decay than for plots with
inverted non-linear decay or standard size, and for errors being higher for scatterplots
with standard size than for plots with inverted non-linear decay. 

Testing for contrasts between the four levels of the size manipulation condition 
were performed with the **emmeans** package (version 1.8.5 \cite{emmeans}), and
are displayed in Table \ref{tab:contrasts-table}

```{r contrasts-table, echo=FALSE, warning=FALSE, message=FALSE}
# outputs summary statistics

contrasts <- contrasts_extract(size_model) %>%
  mutate(p.value = scales::pvalue(p.value)) %>%
  mutate(Contrast = recode(Contrast,
                           "D - C" = "Standard Size : Inverted Non-linear Decay",
                           "C - A" = "Inverted Non-linear Decay : Non-linear Decay",
                           "A - B" = "Non-linear Decay: Linear Decay",
                           "D - A" = "Standard Size : Non-linear Decay",
                           "D - B" = "Standard Size : Linear Decay",
                           "C - B" = "Inverted Non-linear Decay : Linear Decay"))

k <- knitr::kable(contrasts, booktabs = TRUE, digits = c(0,2,3), caption = "contrasts table", escape = FALSE)

kable_styling(k, latex_options = "scale_down")
```

# Discussion

```{r changes-with-r-size, warning=FALSE, echo=FALSE, message=FALSE, fig.align='left', fig.show='hold', fig.cap="hello", crop = TRUE, fig.env="figure*"}
plot_error_bars_function(exp_size_only_tidy, "difference", labels_size) +
    labs(y = "Mean r estimation error") +
    theme(title = element_text(size = 8)) +
    geom_hline(yintercept = 0, linetype = 2)
```


























